<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Systematic Reflections on Current Work &#183; Alex's Blog</title><link rel=stylesheet href=/posts/css/style.css><link rel=stylesheet href=/posts/css/fonts.css><link rel=icon href=favicon.ico><link rel=icon type=image/png sizes=32x32 href=/posts/images/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/posts/images/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/posts/images/apple-touch-icon.png><link href rel=alternate type=application/rss+xml title="Alex's Blog"></head><body><nav class=nav><div class=nav-container><a href=/posts/><h2 class=nav-title>Alex's Blog</h2></a><ul></ul></div></nav><main><div class=post><div class=post-info><span>Written by</span>
Alex<br><span>on&nbsp;</span><time datetime="2022-04-02 20:16:00 +1100 +1100">April 2, 2022</time></div><h1 class=post-title>Systematic Reflections on Current Work</h1><div class=post-line></div><p>After several years of working in the IT industry, I have not systematically reflected on my work during these years. I think it is a good time for me to review my work during these years. This article mainly discusses my summary and thoughts on my work over the years, starting from several common interview questions.</p><h2 id=introduction-to-business-background>Introduction to Business Background</h2><h4 id=companys-main-business-business-logic-technical-difficulties>Company&rsquo;s main business (business logic, technical difficulties)</h4><p>The company&rsquo;s main business involves stock information such as stock prices, news, related financial reports, and shareholder information, as well as paid announcement translations. Based on this, the company provides expanded services such as new stock issuance and fund subscriptions. From a logical perspective, there are mainly two parts to the business. First, data is gathered from various sources. Second, the data is stored and efficiently and reliably displayed to users. Personally, I am mainly responsible for the development and operation of the backend applications, so the technical problems and difficulties I encounter are mainly concentrated in this area. Specifically, this involves how to efficiently and reliably display data to users in a timely manner.</p><h2 id=architecture-evolution>Architecture Evolution</h2><h4 id=how-to-upgrade-the-architecture-servers-applications-databases>How to upgrade the architecture (servers, applications, databases)</h4><p>We use AWS as our cloud service provider. At the beginning, we only had two servers, one for the front-end web and one for the back-end API. All APIs were centralized on a single EC2 instance. Then the back-end server connected to AWS RDS to access data. As the number of services grew, having a single server was no longer feasible. First, as the number of services increased, the server became overloaded and unresponsive. Second, if a problem occurred, the entire website would be inaccessible, causing a significant impact. In order to reduce response time and improve availability, it was inevitable to split services and separate different services, just as breaking down a large function into many small functions. It transitioned from a single-machine mode to a distributed mode step by step.</p><p>One of the earliest services to be split was the static file download service. Initially, some stock announcements were provided through an Nginx server for download service. Although Nginx is powerful, it still has a single point of failure and high storage costs. Later, the PDF files were placed on AWS S3, which improved reliability and download speed, while also reducing storage costs. S3 also provides a storage period for PDFs, automatically migrating less frequently accessed files to slower but cheaper storage locations, achieving a good balance of performance and cost.</p><p>In addition to EC2 instances, we also tried serverless services and migrated some relatively independent APIs to Lambda. Due to the characteristic of only being charged when running, Lambda greatly reduces costs and is suitable for APIs that are not frequently called. For example, the API for verifying the existence of an email is relatively independent, easily called by different services, but not frequently called, making it suitable for migration to Lambda.</p><p>Finally, some services were migrated to Kubernetes clusters. Kubernetes is a very powerful toolset for managing virtual containers. Based on containerization of services, the deployment and management experience of the entire service became very good. Services can be further subdivided, and with the help of Kubernetes, management becomes very convenient. Of course, with the subdivision of services, management also presents challenges, one of which is monitoring a large number of containers. When we only had a few servers before, we only needed to install netdata monitoring tools on each server. Netdata is a very open-source monitoring tool that comes with many monitoring items, such as disk, memory, CPU usage, etc., and is easy to install and use. However, later on, especially after switching to Kubernetes, this method was no longer feasible and evolved into the Prometheus + Grafana + Alertmanager monitoring stack.</p><h2 id=understanding-the-principles-of-the-underlying-technologies-of-the-company>Understanding the principles of the underlying technologies of the company</h2><h4 id=servers-middleware-caching-message-queues-databases-docker-kubernetes-and-so-on>servers, middleware (caching, message queues), databases, Docker, Kubernetes, and so on.</h4><p>Nginx is the open source server program that I first used and studied. I previously studied its C source code and understood its code structure. My impression of nginx is that it uses an asynchronous response model rather than Apache&rsquo;s multi-threaded model. In addition, nginx is known for its high performance and reliability, but it can be relatively difficult to scale and requires recompilation. When changing settings, a restart is required and it cannot be configured through an API. Of course, this was a few years ago, and there should be many extensions and improvements now. However, because nginx is written in C, a lower-level language, its readability is not as good as servers written in Go that have emerged in recent years.</p><p>For databases, I use AWS&rsquo;s RDS databases more often, primarily Postgresql. RDS turns databases into a service and provides easy-to-use services such as automatic backup, master/slave architecture, and powerful monitoring. Another point is that AWS reminds users of security updates for databases, which is also very helpful. AWS also optimizes databases, so as a user, I don&rsquo;t need to pay attention to the hardware needed to run the database, but rather focus on designing tables in the database and setting indexes reasonably. For example, as the table grows, it may need to be split horizontally or vertically, depending on the pattern of API access to the database, and SQL queries also need to be optimized. I have done some related research on this topic, such as &ldquo;Debugging and optimizing SQL (Postgres query planner).&rdquo;</p><p>Redis is a caching middleware that I really like to use. It is an open source caching tool developed in C, which uses very little system resources but has very powerful features. Redis supports multiple data structures and is constantly adding more. Redis can handle tens of thousands of connections simultaneously and has fast response times, in milliseconds or even microseconds. I still use Redis sparingly, mainly focusing on single instances and have not yet used its clustering capabilities. Although Redis supports multiple connections at the same time, it is best for client applications to use a connection pool to reduce the time it takes to establish a connection, which can greatly improve response time. Also, using the exponential backoff algorithm to implement timeout reconnections can reduce the pressure on Redis servers, which also applies to related services such as databases.</p><p>Docker is one of the tools I use the most. It was a bit difficult to learn at first, but once I mastered it, I couldn&rsquo;t do without it. Docker not only provides great convenience for testing and development, such as saving time setting up local services and easily setting up development environments in different environments as long as Docker is installed, but also makes deployment very easy after containerization, without worrying about running successfully on local machines but not on production servers. In short, Docker greatly improves development and deployment efficiency and is an essential tool for DevOps.</p><p>Kubernetes is a tool I have used a lot in recent years. Especially when there are a lot of service splits and containerization, I mainly use the fargate mode of EKS. In fargate mode, AWS will be responsible for server management, and we do not need to configure the server in advance. We only need to configure the required CPU and memory when running the container, which improves resource utilization.</p><p>Prometheus+Grafana+Alertmanager are cluster monitoring tools that have been very popular in DevOps in recent years. Because of their open source and powerful community, they are powerful, easy to expand, and even many large companies like Cloudflare use them. They can be used to monitor infrastructure and application running status, which is very convenient. Currently, we use this combination of monitoring tools on our Kubernetes cluster. The most commonly used feature is notifying the running status of pods, such as restart notifications or autoscale notifications.</p><p>When it comes to DevOps, Terraform, a very useful tool, cannot be ignored. Using it can achieve infrastructure as code, greatly reducing maintenance costs. In some ways, this tool is similar to Kubernetes, using descriptive YAML files to control infrastructure without needing to know how to implement it, just describing what you want in a language that Terraform understands. This is somewhat like a higher level of abstraction than Ansible. In Ansible, you need to write specific implementations, while in Terraform, you don&rsquo;t need to.</p><h2 id=examination-of-system-challenges>Examination of System Challenges</h2><h4 id=technical-difficulties-what-are-they-why-are-they-difficult-and-how-to-solve-them>Technical difficulties (what are they, why are they difficult, and how to solve them).</h4><p>The most difficult technical challenge I encountered was upgrading and optimizing a GraphQL server application. This problem had been troubling me for quite some time. Initially, the infrastructure server crashed, causing the CPU to be constantly overloaded, and eventually crashing under the strain. The first time, I upgraded the machine&rsquo;s configuration, temporarily solving the problem, but after a period of time, the problem reappeared. The code was written by a former colleague who had already left the company, and at first, I thought the problem was due to poorly written code that needed optimization. However, after carefully examining the code, I found that most of it was auto-generated, and if there were any problems, it was likely due to the framework code. After some testing and getting to know the framework, I discovered that a GraphQL query would generate many SQL queries, and the reason for the CPU spike was due to the database fetching part. The more data the database had, the larger the query data, the more CPU usage, and the slower the response.</p><p>At the time, due to time constraints, I didn&rsquo;t investigate the root cause, but naturally thought of using caching to reduce the number of queries to the database. Fortunately, the Apollo server had a corresponding caching plugin, which I modified slightly and used, and it greatly reduced the CPU load. So everything was fine for a long time, and I even thought the problem would never reappear, but it did.</p><p>Although it didn&rsquo;t crash, the speed slowed down a lot, especially when the cache was invalidated, and it started to become unsustainable. This time, I finally decided to take the time to dive deeper into the framework. The entire framework was written in JavaScript, which didn&rsquo;t seem very convenient. After many experiments and reading the code, I finally discovered the root cause of the slow service. Simply put, during the process of converting GraphQL queries to SQL queries, the framework&rsquo;s adaptability caused some queries to be inefficient, resulting in the need to fetch many useless data, which not only increased the burden on the database and transmission time but also significantly increased CPU consumption on the server. Knowing the reason, I took targeted measures and placed some restrictions on the queries. Although this had some side effects, causing some queries to not work properly, after researching and testing, it did not affect the existing queries, and the changes were few and easy to implement. The most important benefit was that after the changes, the speed would not be affected even if the database added more data, which could be considered a cure for the problem. So after weighing the options, I used this method.</p></div><div class=pagination><a href=/posts/sql/ class="left arrow">&#8592;</a>
<a href=/posts/grapqlloader/ class="right arrow">&#8594;</a>
<a href=# class=top>Top</a></div></main><footer><span>&copy; <time datetime="2023-07-12 12:46:40.449242386 +0000 UTC m=+0.052779108">2023</time> Alex. Made with <a href=https://gohugo.io>Hugo</a> using the <a href=https://github.com/EmielH/tale-hugo/>Tale</a> theme.</span></footer></body></html>