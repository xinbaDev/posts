<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Debugging and Optimization of SQL (Postgres query planner) &#183; Alex's Blog</title><link rel=stylesheet href=/posts/css/style.css><link rel=stylesheet href=/posts/css/fonts.css><link rel=icon href=favicon.ico><link rel=icon type=image/png sizes=32x32 href=/posts/images/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/posts/images/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/posts/images/apple-touch-icon.png><link href rel=alternate type=application/rss+xml title="Alex's Blog"></head><body><nav class=nav><div class=nav-container><a href=/posts/><h2 class=nav-title>Alex's Blog</h2></a><ul></ul></div></nav><main><div class=post><div class=post-info><span>Written by</span>
Alex<br><span>on&nbsp;</span><time datetime="2023-01-13 19:33:00 +1100 +1100">January 13, 2023</time></div><h1 class=post-title>Debugging and Optimization of SQL (Postgres query planner)</h1><div class=post-line></div><p>Recently, I encountered an interesting SQL problem at work. Almost the same raw SQL resulted in execution times that differed by more than a hundredfold.</p><p>The problematic raw SQL (simplified for readability) is as follows:</p><pre><code># Returns the price data for all popular stocks from 10am (opening) to 4pm (closing).
SELECT &quot;marketdata&quot;.&quot;id&quot;, 
       &quot;marketdata&quot;.&quot;code_id&quot;, 
       &quot;marketdata&quot;.&quot;date&quot;,  
       &quot;marketdata&quot;.&quot;price&quot;,  
FROM &quot;marketdata&quot; INNER JOIN &quot;stock&quot; ON 
(&quot;marketdata&quot;.&quot;code_id&quot; = &quot;stock&quot;.&quot;code&quot;) 
WHERE (
    &quot;stock&quot;.&quot;is_hot&quot; = true AND 
    &quot;marketdata&quot;.&quot;date&quot; BETWEEN 'XXXX-XX-XXT10:00:00' AND 'XXXX-XX-XXT16:00:00') 
ORDER BY &quot;marketdata&quot;.&quot;date&quot; ASC
</code></pre><h2 id=0x01-background>0x01 Background</h2><p>Our company mainly focuses on researching and analyzing Australian stock market information, including publishing prices, trends, announcements, and other important market information for all stocks listed on the Australian Securities Exchange (ASX). Recently, due to project requirements, we added a new category for stocks called &ldquo;special&rdquo;.</p><p>Previously, there were already many categories, such as popular stocks, gold mining stocks, etc. So adding a new category was not something new. All we had to do was to add a category to the stock table in the database (by adding an &ldquo;is_special&rdquo; column to the stock schema) and then add an &ldquo;is_special&rdquo; filter to the corresponding API. However, this seemingly simple requirement taught me a lot of things :)</p><p>As soon as the new feature was deployed to the staging environment, we found that the response time of the corresponding API was much longer than that of other APIs of the same type. The normal APIs returned responses in less than 1 second, but this new API required a shocking 30-40 seconds, or even timed out. However, there was no problem when testing locally. We quickly ran a git diff command to confirm that there were no basic code logic errors. We soon ruled out code issues that could cause long responses (as there were very few changes), but during a quick code review, we noticed that whenever the cache expired, the API would read the corresponding data from the database and write it to the cache. Based on the previous test results, we immediately suspected that there was a problem on the database side. We looked at the AWS monitor and found that the number of database reads had skyrocketed since the deployment, confirming that there was a problem on the database side. My first guess was that an index was missing, but that was quickly ruled out after inspection. Later, using some debugging tools, we obtained the raw SQL generated by the ORM (as shown above). We found that is_hot was working normally, but is_special was very slow. Thus, the cause of the problem had been narrowed down to a relatively small scope, and the variable that could cause the error had been greatly reduced. The remaining problem was:</p><p>Why is &ldquo;is_special&rdquo; so slow compared to &ldquo;is_hot&rdquo;?</p><h2 id=0x02-preliminary-analysis-of-slow-db-queries>0x02 Preliminary Analysis of Slow DB Queries</h2><p>First, let&rsquo;s take a look at what is different between these two queries. At first glance, it&rsquo;s just that &ldquo;is_hot&rdquo; has been changed to &ldquo;is_special&rdquo;. However, at the database level, the DB will read all stocks where &ldquo;is_special&rdquo; or &ldquo;is_hot&rdquo; is true. &ldquo;is_special&rdquo; is a new category with only two stocks, so reading all the data will only result in 360 (the number of price data entries per day) * 2 = 720 rows. On the other hand, &ldquo;is_hot&rdquo; represents popular stocks, which there are hundreds of, resulting in 360 × 100 = 36,000 rows. Why is it that the SQL query with the larger amount of data actually executes faster than the one with the smaller amount of data, despite the fact that they are essentially the same query?</p><p>This has to do with how Postgres executes this SQL statement. As we have previously learned, the most important step in the database&rsquo;s processing of a query is planning. In the case of Postgres, when the DB receives an SQL query, it will estimate the time required for all possible combinations of query execution plans (such as how to scan, how to join, how to order) based on some previous statistical data, and then find the execution plan that requires the least amount of time, and then execute it. How can we check how the DB estimates and finds the optimal solution? Postgres has a command called EXPLAIN ANALYZE.</p><p>Next, let&rsquo;s take a look at how these two &ldquo;sibling&rdquo; queries are executed in the DB.
First take a look at &ldquo;is_hot&rdquo;</p><pre><code>Sort  (cost=119007.54..119066.60 rows=23621 width=241) (actual time=116.795..127.942 rows=37125 loops=1)&quot;
  Sort Key: marketdata.date&quot;
  Sort Method: external merge  Disk: 4800kB&quot;
  -&gt;  Nested Loop  (cost=24.31..114544.24 rows=23621 width=241) (actual time=0.185..73.446 rows=37125 loops=1)&quot;
        -&gt;  Seq Scan on stock  (cost=0.00..441.12 rows=100 width=4) (actual time=0.066..1.211 rows=100 loops=1)&quot;
              Filter: is_hot&quot;
              Rows Removed by Filter: 2492&quot;
        -&gt;  Bitmap Heap Scan on marketdata  (cost=24.31..1138.09 rows=294 width=241) (actual time=0.107..0.503 rows=371 loops=100)&quot;
              Recheck Cond: (((code_id)::text = (stock.code)::text) AND (date &gt;= 'XXXX-XX-XX 00:00:00.331101+00'::timestamp with time zone) AND (date &lt;= 'XXXX-XX-XX 08:08:55.331109+00'::timestamp with time zone))&quot;
              Heap Blocks: exact=37119&quot;
              -&gt;  Bitmap Index Scan on marketdata_code_id_date_e4afd606_idx  (cost=0.00..24.23 rows=294 width=0) (actual time=0.068..0.068 rows=371 loops=100)&quot;
                    Index Cond: (((code_id)::text = (stock.code)::text) AND (date &gt;= 'XXXX-XX-XX 00:00:00.331101+00'::timestamp with time zone) AND (date &lt;= 'XXXX-XX-XX 08:08:55.331109+00'::timestamp with time zone))&quot;
Planning time: 0.704 ms&quot;
Execution time: 135.848 ms&quot;

</code></pre><p>generated by(<a href=https://tatiyants.com/pev>pev</a>）:</p><p><img src=https://i.imgur.com/okyfgCI.png alt></p><p>and the &ldquo;is_special&rdquo;</p><pre><code>Sort  (cost=725081.36..725852.57 rows=308485 width=241) (actual time=44276.683..44276.954 rows=1125 loops=1)&quot;
  Sort Key: marketdata.date&quot;
  Sort Method: quicksort  Memory: 250kB&quot;
  -&gt;  Hash Join  (cost=457.44..625254.48 rows=308485 width=241) (actual time=37102.527..44274.947 rows=1125 loops=1)&quot;
        Hash Cond: ((marketdata.code_id)::text = (stock.code)::text)&quot;
        -&gt;  Seq Scan on marketdata  (cost=0.00..619398.55 rows=616970 width=241) (actual time=36321.218..44015.861 rows=783850 loops=1)&quot;
              Filter: ((date &gt;= 'XXXX-XX-XX 00:00:00.331101+00'::timestamp with time zone) AND (date &lt;= 'XXXX-XX-XX 08:08:55.331109+00'::timestamp with time zone))&quot;
              Rows Removed by Filter: 18213401&quot;
        -&gt;  Hash  (cost=441.12..441.12 rows=1306 width=4) (actual time=4.070..4.070 rows=3 loops=1)&quot;
              Buckets: 2048  Batches: 1  Memory Usage: 17kB&quot;
              -&gt;  Seq Scan on stock  (cost=0.00..441.12 rows=1306 width=4) (actual time=4.059..4.064 rows=3 loops=1)&quot;
                    Filter: is_special&quot;
                    Rows Removed by Filter: 2589&quot;
Planning time: 0.692 ms&quot;
Execution time: 44277.220 ms&quot;
</code></pre><p><img src=https://i.imgur.com/n9FBnAJ.png alt></p><p>As you can see, two almost identical queries have vastly different execution times in the database. Both &ldquo;is_hot&rdquo; and &ldquo;is_special&rdquo; perform sorting at the end. Because the data volume of &ldquo;is_hot&rdquo; exceeds the memory limit and quicksort algorithm is used in external merge to sort the data on the hard disk. Whereas the data volume of &ldquo;is_special&rdquo; is small and can be completely sorted in memory using quicksort algorithm. Normally, disk operations are much slower than memory operations (hundreds of times slower), so &ldquo;is_special&rdquo; should be faster than &ldquo;is_hot,&rdquo; but the opposite is true in the end. This indicates that there are some time-consuming operations in &ldquo;is_special&rdquo; before sorting. Continuing to look down, we will soon discover a particularly time-consuming operation in &ldquo;is_special&rdquo;. &ldquo;Seq Scan on marketdata (cost=0.00..619398.55 rows=616970 width=241) (actual time=36321.218..44015.861 rows=783850 loops=1).&rdquo; This operation will traverse and read the data of the entire table, which according to Postgres' estimate, has 616,970 rows (actually 783,850 rows). It is inevitable that it will take a long time to sequentially traverse such a large table. Ultimately, this operation greatly prolongs the execution time of the entire query. In contrast, in the case of &ldquo;is_hot,&rdquo; the scan method used for this large table is first a bitmap index scan, followed by a bitmap heap scan, which greatly reduces the query time compared to the first method.</p><p>By analyzing this, the root cause of the problem has been narrowed down further. It can be basically determined that the Postgres query planner has a problem and has not found the optimal solution. The next question is, why did the Postgres query planner make this decision?</p><h2 id=0x03-further-analysis-of-sql-query-planner>0x03 Further analysis of SQL query planner</h2><p>Firstly, it can be seen that in the query for is_special, Postgres produced a serious estimation bias for the number of rows with is_special=True in the table (estimated 1306 rows, but only 3 rows actually exist). However, in the is_hot query, Postgres made a correct estimation (estimated 100 rows and there are actually 100 rows).</p><p>This estimation error directly caused the estimation time of bitmap heap scan and Nested Loop to be much higher than the actual values. The bitmap heap scan looped 3 times, but with the wrong estimation, it had to loop 1306 times. The Nested Loop went from 3 × 360 × 3 times to 1306 × 360 × 1306 times. As a result, the query planner had to choose another plan that was very time-consuming, but had less estimated time.</p><p>Why did Postgres produce such a large estimation bias for the number of rows with is_special=True? This involves how Postgres estimates the number of rows involved in a query. After checking the official Row Estimation documentation, it was found that there is a concept called MCV (most common values) in PG. In short, Postgres collects statistical data about MCV for a column in a view called pg_stats, and uses this data to estimate the number of rows involved in related queries. Following the example in the official documentation, we can look up the statistical data for the is_special column in the stock table.</p><pre><code>SELECT null_frac, n_distinct, most_common_vals, most_common_freqs
       FROM pg_stats
       WHERE tablename='stock' and attname='is_special';
  
</code></pre><p>return:</p><pre><code> null_frac | n_distinct | most_common_vals | most_common_freqs 
-----------+------------+------------------+-------------------
(0 rows)

</code></pre><p>The expected error statistics were not found and a null value was returned, indicating that the database does not have the relevant statistics. This raises two questions:</p><p>(1) Who generates the statistics and why are they missing?
(2) If there are no statistics, how did Postgres estimate the number of rows with is_special=True?</p><p>Firstly, according to the official documentation, statistics are generated by an Autovacuum Daemon. This daemon is very important as it not only generates statistics but also performs the following tasks:</p><ul><li>To recover or reuse disk space occupied by updated or deleted rows.</li><li>To update the visibility map, which speeds up index-only scans.</li><li>To protect against loss of very old data due to transaction ID wraparound or multixact ID wraparound.</li></ul><p>Since these tasks are not directly related to the topic, I will not go into detail here, but they are still very important. In the same document, it is stated that:</p><p>For analyze, a similar condition is used: the threshold, defined as:</p><p>analyze threshold = analyze base threshold + analyze scale factor * number of tuples</p><p>Immediately following this, it is stated that:</p><p>The default thresholds and scale factors are taken from postgresql.conf&mldr;</p><p>In postgresql.conf:</p><p>#autovacuum_analyze_threshold = 50 # min number of row updates before analyze
#autovacuum_analyze_scale_factor = 0.1 # fraction of table size before analyze</p><p>No wonder there were no relevant statistics, because the threshold had not been reached.</p><p>As for the second question, the official documentation does not provide a specific explanation of how row counts are estimated when statistics are not available.</p><p>To find out, I had to look into the source code of Postgres. I started from the cost_seqscan function in optimizer/path and traced the functions that use statistics all the way to the booltestsel function in selfuncs.c.</p><pre><code>/*
 * booltestsel		- Selectivity of BooleanTest Node.
 */
 
Selectivity
booltestsel(PlannerInfo *root, BoolTestType booltesttype, Node *arg,
			int varRelid, JoinType jointype, SpecialJoinInfo *sjinfo)
{

    ...

	else
	{
		/*
		 * If we can't get variable statistics for the argument, perhaps
		 * clause_selectivity can do something with it.  We ignore the
		 * possibility of a NULL value when using clause_selectivity, and just
		 * assume the value is either TRUE or FALSE.
		 */
		switch (booltesttype)
		{
			case IS_UNKNOWN:
				selec = DEFAULT_UNK_SEL;
				break;
			case IS_NOT_UNKNOWN:
				selec = DEFAULT_NOT_UNK_SEL;
				break;
			case IS_TRUE:
			case IS_NOT_FALSE:
				selec = (double) clause_selectivity(root, arg, varRelid, jointype, sjinfo);
				break;
			case IS_FALSE:
			case IS_NOT_TRUE:
				selec = 1.0 - (double) clause_selectivity(root, arg, varRelid, jointype, sjinfo);
				break;
			default:
				elog(ERROR, &quot;unrecognized booltesttype: %d&quot;,
					 (int) booltesttype);
				selec = 0.0;	/* Keep compiler quiet */
				break;
		}
	}


</code></pre><p>According to the comments, for boolean values, if the db cannot find statistical information, it will exclude null values and estimate based on the assumption that 50% are trur and 50% are false. Based on the 2598 rows in the stock table, the estimated number of rows with is_special=True should be around 1300. This is consistent with the situation during EXPLAIN ANALYZE.</p><h2 id=0x04-solution>0x04 Solution</h2><p>According to the documentation, manually analyze the column of the relevant table, force update the data in pg_stats, so that the query planner can make planning based on accurate data. Alternatively, adjust the threshold value for the stock table so that the autovacuum daemon can do the work, which can also solve the problem.</p><p>Another approach is to directly tell the query planner the codes involved, so that the query planner does not have to guess, and the optimal solution can be obtained. Personally, I prefer to use this method. The advantage is that it is easy to migrate, and there is no need to adjust the new postgres when migrating to other databases. Another advantage is to reduce the loading of the query planner, allowing the query planner to find the best execution path more stably.</p><h2 id=0x05-reflection>0x05 Reflection</h2><p>During this debugging process, I spent a considerable amount of time searching for information and conducting various tests, which has led me to gain a much deeper understanding of how postgres operates and can be optimized. One key insight I&rsquo;ve discovered is that while the postgres query planner is certainly capable, as programmers we shouldn&rsquo;t rely on it to guess our intentions all the time. By delving into the source code and conducting thorough research, I&rsquo;ve been able to gain a much clearer picture of how postgres operates and how it can be optimized to its fullest potential. Overall, this experience has allowed me to shed light on what was once a black box for me, and has given me a much deeper understanding of the intricacies of database optimization.</p><h2 id=reference>Reference</h2><ol><li><a href=https://thoughtbot.com/blog/advanced-postgres-performance-tips>https://thoughtbot.com/blog/advanced-postgres-performance-tips</a></li><li><a href=https://www.postgresql.org/docs/9.3/row-estimation-examples.html>https://www.postgresql.org/docs/9.3/row-estimation-examples.html</a></li><li><a href=https://gocardless.com/blog/debugging-the-postgres-query-planner>https://gocardless.com/blog/debugging-the-postgres-query-planner</a></li><li><a href=https://blog.dbi-services.com/are-statistics-immediately-available-after-creating-a-table-or-an-index-in-postgresql/>https://blog.dbi-services.com/are-statistics-immediately-available-after-creating-a-table-or-an-index-in-postgresql/</a></li><li><a href=https://www.postgresql.org/docs/10/routine-vacuuming.html>https://www.postgresql.org/docs/10/routine-vacuuming.html</a></li><li><a href=https://docs.mongodb.com/manual/tutorial/analyze-query-plan/>https://docs.mongodb.com/manual/tutorial/analyze-query-plan/</a></li><li><a href=https://thoughtbot.com/blog/reading-an-explain-analyze-query-plan>https://thoughtbot.com/blog/reading-an-explain-analyze-query-plan</a></li><li><a href=https://www.smartly.io/blog/letting-postgresql-plan-well-for-you-markus-winand>https://www.smartly.io/blog/letting-postgresql-plan-well-for-you-markus-winand</a></li><li><a href=https://blog.csdn.net/kmblack1/article/details/80761647>https://blog.csdn.net/kmblack1/article/details/80761647</a></li><li><a href=https://www.cnblogs.com/kissdodog/p/3160560.html>https://www.cnblogs.com/kissdodog/p/3160560.html</a></li><li><a href=https://docs.mongodb.com/manual/core/query-plans/>https://docs.mongodb.com/manual/core/query-plans/</a></li></ol></div><div class=pagination><a href=/posts/server/ class="left arrow">&#8592;</a>
<a href=/posts/chatgpt/ class="right arrow">&#8594;</a>
<a href=# class=top>Top</a></div></main><footer><span>&copy; <time datetime="2023-10-24 15:09:07.339993768 +0000 UTC m=+0.055816260">2023</time> Alex. Made with <a href=https://gohugo.io>Hugo</a> using the <a href=https://github.com/EmielH/tale-hugo/>Tale</a> theme.</span></footer></body></html>